{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrey-Epifanov-k721/conv_networks/blob/master/object_detection_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV66G0WGGuBj"
      },
      "source": [
        "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=500></p>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LuFLCm3GuBl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7pPmypAGuBm"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Object detection</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLt9nwPoGuBn"
      },
      "source": [
        "### Руководитель проекта:\n",
        "* Юрий Яровиков (AIRI, МФТИ) | tg:@yu_rovikov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwjXeeaDGuBp"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Треки на проекте</b></h1>\n",
        "На этом проекте есть два возможных трека, из которых нужно выбрать один.\n",
        "\n",
        "* **Первый трек --- исследовательский**. На этом треке вам предстоит самостоятельно обучить и протестировать предобученную модель детекции. Основной упор делается на моделирование и обучение. Необходимо будет попробовать несколько моделей детекции, самостоятельно реализовать метрики.\n",
        "\n",
        "* **Второй трек --- продуктовый**. На этом треке вам не понадобится обучать свою модель детекции (хотя никто не запрещает вам это делать), но необходимо, во-первых, продумать **продуктовую составляющую проекта** (проблема людей, которая решается в данном проекте, целевая аудитория продукта, оптимальный способ внедрения модели), а также создать [MVP](https://ru.wikipedia.org/wiki/%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE_%D0%B6%D0%B8%D0%B7%D0%BD%D0%B5%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%B4%D1%83%D0%BA%D1%82) , **внедрив модель в цифровой сервис**, который может быть реализован как Telegram-бот, Web-демо, Desktop-приложение.\n",
        "\n",
        "Вам необходимо выбрать основной сценарий, по которому вы пойдете, указав это при сдаче работы. При этом, никто не мешает вам совместить два трека, проведя и моделирование, и встраивание в демо. В этом случае мы рекомендуем пойти по **плану из второго трека**, а за моделирование будут ставиться бонусные баллы.\n",
        "\n",
        "Обратите внимание, что суммарный балл по проекту не может превышать 10. Максимальный балл можно получить на любом из двух треков."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Исследовательский трек\n",
        "На этом треке вам предстоит самостоятельно обучить и протестировать предобученную модель детекции. Основной упор делается на моделирование и обучение. Необходимо попробовать несколько моделей детекции и провести их объективное сравнение в соответствии с целевой метрикой проекта.\n",
        "\n",
        "## План работы\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "58WM1dAoU5cV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Выбор фреймворка/библиотеки для использования детектора (1 балл)\n",
        "\n",
        "Чтобы освежить память о задаче детекции, можно посмотреть [занятия на продвинутом курсе](https://stepik.org/lesson/458312/step/1?unit=616130).\n",
        "\n",
        "В выборе фреймворка предоставляется свобода, лично я рекомендовал бы один из:\n",
        "- `torchvision.models.detection` и `torchhub`: \"нативные\" модели для детектирования прямо из PyTorch. Примеры использования есть прямо на занятиях DLSchool по практике CV [2019 года](https://www.youtube.com/watch?v=XSPYe4-y4HE) и [2020 года](https://stepik.org/lesson/458313/step/1?unit=616131);\n",
        "- `mmdetection`: как с ним работать, рассказывается в [практическом занятии](https://stepik.org/lesson/458313/step/2?unit=616131).\n",
        "- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (\"Quick Start\");\n",
        "- `TensorFlow Object Detection API`: как с ним работать рассказывается в [занятии 2018 года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Работать предстоит с [официальным репозиторием](https://github.com/tensorflow/models/tree/master/research/object_detection).\n",
        "\n",
        "**Обратите внимание, что для получения полного балла по проекту необходимо обучить и сравнить как минимум две различные модели детекции (можно из одного фреймворка)!**\n"
      ],
      "metadata": {
        "id": "U_JHEakt_Bb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch torchvision"
      ],
      "metadata": {
        "id": "9l3mg1D_W4g-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a110ee9-e6d5-4dda-9c39-0374e0266451"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# нам необходима библиотека pillow для преобразования изображений их аугментации и т.д.\n",
        "\n",
        "!pip install Pillow\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "metadata": {
        "id": "n_yEpUhLQ8_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fa0450-f83b-4607-9636-3d1eba1f1fd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "8.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-5d0b526ee0e4>:5: DeprecationWarning: PILLOW_VERSION is deprecated and will be removed in Pillow 9 (2022-01-02). Use __version__ instead.\n",
            "  print(PIL.PILLOW_VERSION)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "metadata": {
        "id": "_egIS0QyO846",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716c9d13-10bd-4738-b07a-a5908e868879"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.  Training on CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "from torchvision import transforms\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from matplotlib import colors, pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# в sklearn не все гладко, чтобы в colab удобно выводить картинки\n",
        "# мы будем игнорировать warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "MEQhro_RRdBa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90z-NuSbGuB5"
      },
      "source": [
        "### 2. Выбор датасета (0 баллов)\n",
        "\n",
        "Вы можете выбрать любой датасет для детекции. Вот несколько идей:\n",
        "1. [Детекция игровых карт](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10): лежат в папке images вместе с разметкой;\n",
        "2. [Детекция фруктов](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection): скачать можно, нажав на кнопку Download;\n",
        "3. [Детекция одежды (Deep Fashion 2)](https://github.com/switchablenorms/DeepFashion2): стоит прочитать README на главной странице репозитория. Для получения датасета нужно запросить пароль у автора через гугл-форму. После скачивания распакуйте его с использованием пароля. Из файлов аннотаций нас будут интересовать только `bounding_box`, `category_name` и `category_id`;\n",
        "4. [Детекция лиц (Wider Face)](http://shuoyang1213.me/WIDERFACE/): большой датасет для детектирования лиц самых разных размеров. Скачать можно прямо по ссылкам на сайте;\n",
        "5. [Детекция лиц (Kaggle)](https://www.kaggle.com/dataturks/face-detection-in-images): в датасете достаточно мало данных, но можно попробовать, если датасеты выше показались неподходящими для Вас;\n",
        "6. Датасет из любого соревновани по детекции на Kaggle.\n",
        "\n",
        "При работе с датасетом вы неизбежно столкнетесь с работой с файлами и папками (директориями). Рекомендуется освежить в памяти работу с библиотеками `os`, `json`, `glob`. Может помочь [этот туториал](https://realpython.com/working-with-files-in-python/).\n",
        "\n",
        "> Результатом выполнения пункта явлется загруженный датасет, состоящий из изображений и разметки к ним (bounding box'ов всех объектов на каждом изображении)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "pXG7mlBOmIQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0688a1b8-8f33-4586-d7ad-83fa91e77799"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/gdrive/MyDrive/archive.zip -d archive"
      ],
      "metadata": {
        "id": "8ymPtVHMYosZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES = Path('/content/archive/images')\n",
        "LABELS = Path('/content/archive/labels')"
      ],
      "metadata": {
        "id": "e_Sig84YZQ0v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Запуск детектора на случайных изображениях (1 балл)\n",
        "\n",
        "В этом пункте вам необходимо применить модель детектирования в выбранном выше репозитории (по сути проверить, что инференс в модели работает). Таким образом, вы убедитесь, что модель работает, и сможет переходить к обучению.\n",
        "\n",
        "> Результатом пункта явлется набор изображений, на которых модель успешно отработала и результат детекции виден и понятен.\n",
        "\n"
      ],
      "metadata": {
        "id": "OkobRpd6W3An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image, ImageDraw"
      ],
      "metadata": {
        "id": "SsrsKoyhbfVI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = list(IMAGES.glob('*.jpg'))"
      ],
      "metadata": {
        "id": "g8Oeu51Jbar4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1 # Количество случайных изображений для вывода\n",
        "random_images = random.sample(image_files, n)"
      ],
      "metadata": {
        "id": "ROH56FrBbW1g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_path in tqdm(random_images):\n",
        "    # Загрузка изображения\n",
        "    image = Image.open(image_path)\n",
        "    image_width, image_height = image.size\n",
        "\n",
        "    # Получение соответствующей метки\n",
        "    label_path = LABELS / (image_path.stem + '.txt')\n",
        "    with open(label_path, 'r') as f:\n",
        "        # Чтение координат боксов из метки\n",
        "        # Предполагается, что данные хранятся в текстовом файле, каждая строка соответствует одному боксу\n",
        "        boxes = [list(map(float, line.strip().split())) for line in f]\n",
        "\n",
        "    # Рисование боксов на изображении\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for box in boxes:\n",
        "        x, y, w, h = box[1:]  # Пропускаем первый элемент, так как он не используется\n",
        "        left = (x - w/2) * image_width\n",
        "        top = (y - h/2) * image_height\n",
        "        right = (x + w/2) * image_width\n",
        "        bottom = (y + h/2) * image_height\n",
        "        draw.rectangle([left, top, right, bottom], outline='red')\n",
        "\n",
        "    # Вывод изображения\n",
        "    image.show()\n"
      ],
      "metadata": {
        "id": "Fpjcv6cSn0XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Yz0JAWGuB6"
      },
      "source": [
        "### 4. Предобработка данных (2 балла)\n",
        "\n",
        "Самый непростой этап в этом сценарии. Скачать данные $-$ лишь половина дела. Чтобы обучить нейросеть на этих данных, нужно написать генератор батчей. Однако если будем подавать изображения так, как они есть, то даже батч собрать не сможем -- нужно привести их к однмоу размеру. Далее нужно привести их к типу float, переместить на CUDA и поделить значения в пикселях на 255 (подробнее см. [занятие](https://www.youtube.com/watch?v=XSPYe4-y4HE)). Также нужно настроить аугментации и постобработку.\n",
        "\n",
        "То, как именно все это реализовать $-$ зависит от инструмента, выбранного в пункте 1. Например, в detectron2 в обучающих материалах описан формат данных для обучения. Возможно, нужно будет зайти в документацию и почитать более подробно, чтобы разобраться, какой именно нужен формат координат.\n",
        "\n",
        "НЕ нужно копировать все файлы с картинками и разметкой прямо на диске в их предобработанные версии. Хороший тон $-$ осуществлять всю эту обработку программно, \"на лету\". Поможет [туториал](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) по написанию своего датасета на PyTorch.\n",
        "\n",
        "> Результатом выполнения пункта явлется код, запуск которого ведет к подаче батчей правильного вида (разметка приведена к требуемому формату координат, изображения нужного типа, размера и поделены на 255 и т.д.) для обучения нейронной сети-детектора."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Инициализация переменных для хранения максимальных и минимальных размеров\n",
        "max_width, max_height = 0, 0\n",
        "min_width, min_height = float('inf'), float('inf')\n",
        "\n",
        "# Инициализация массивов для хранения индексов изображений с максимальными и минимальными размерами\n",
        "max_size_indices = []\n",
        "min_size_indices = []\n",
        "\n",
        "# Перебор изображений в директории\n",
        "for i, image_file in tqdm(enumerate(os.listdir(IMAGES))):\n",
        "    image_path = IMAGES / image_file\n",
        "\n",
        "    # Загрузка изображения\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Получение размеров изображения\n",
        "    width, height = image.size\n",
        "\n",
        "    # Обновление максимальных и минимальных размеров\n",
        "    max_width = max(max_width, width)\n",
        "    max_height = max(max_height, height)\n",
        "    min_width = min(min_width, width)\n",
        "    min_height = min(min_height, height)\n",
        "\n",
        "    # Проверка, является ли текущее изображение максимальным или минимальным по размеру\n",
        "    if width == max_width and height == max_height:\n",
        "        max_size_indices.append(i)\n",
        "    if width == min_width and height == min_height:\n",
        "        min_size_indices.append(i)\n",
        "\n",
        "# Вывод максимальных и минимальных размеров на экран\n",
        "print(\"Максимальные размеры изображения:\")\n",
        "print(\"Ширина:\", max_width)\n",
        "print(\"Высота:\", max_height)\n",
        "print()\n",
        "print(\"Минимальные размеры изображения:\")\n",
        "print(\"Ширина:\", min_width)\n",
        "print(\"Высота:\", min_height)\n",
        "print()\n",
        "print(\"Кол-во изображений с минимальными размерами: \", len(min_size_indices))\n",
        "print(\"Кол-во изображений с максимальными размерами: \", len(max_size_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o-RvphRbyW_",
        "outputId": "34d17c05-fe05-4643-caa7-d5e8219952a9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12880it [00:02, 4732.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Максимальные размеры изображения:\n",
            "Ширина: 1024\n",
            "Высота: 9108\n",
            "\n",
            "Минимальные размеры изображения:\n",
            "Ширина: 1024\n",
            "Высота: 171\n",
            "\n",
            "Кол-во изображений с минимальными размерами:  9\n",
            "Кол-во изображений с максимальными размерами:  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод изображений с максимальными размерами\n",
        "print(\"Изображения с максимальными размерами:\")\n",
        "for index in max_size_indices:\n",
        "    image_file = os.listdir(IMAGES)[index]\n",
        "    image_path = IMAGES / image_file\n",
        "    image = Image.open(image_path)\n",
        "    image.show()"
      ],
      "metadata": {
        "id": "tqheagr4cIsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод изображений с минимальными размерами\n",
        "print(\"Изображения с минимальными размерами:\")\n",
        "for index in min_size_indices:\n",
        "    image_file = os.listdir(IMAGES)[index]\n",
        "    image_path = IMAGES / image_file\n",
        "    image = Image.open(image_path)\n",
        "    image.show()\n"
      ],
      "metadata": {
        "id": "lDxWxUcGcHMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_files = sorted(list(image_dir.glob('*.jpg')))\n",
        "        self.label_files = sorted(list(label_dir.glob('*.txt')))\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Изменение размера изображения\n",
        "            transforms.CenterCrop((256, 256)),  # Обрезка изображения до заданного размера\n",
        "            transforms.ToTensor(),  # Преобразование в тензор\n",
        "            transforms.Normalize((0.0,), (1.0,))  # Нормализация значений пикселей\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_files[index]\n",
        "        label_path = self.label_files[index]\n",
        "\n",
        "        # Загрузка изображения\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = self.transform(image).float()  # Преобразование в тип float\n",
        "        image = image / 255.0  # Деление значений пикселей на 255\n",
        "\n",
        "        # Перемещение изображения на устройство CUDA (если доступно)\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.cuda()\n",
        "\n",
        "        # Загрузка меток боксов\n",
        "        with open(label_path, 'r') as f:\n",
        "            boxes = [list(map(float, line.strip().split())) for line in f]\n",
        "\n",
        "        return image, boxes\n",
        "\n",
        "\n",
        "def get_data(image_dir, label_dir):\n",
        "    dataset = CustomDataset(image_dir, label_dir)\n",
        "    return dataset\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    boxes = []\n",
        "\n",
        "    for image, label in batch:\n",
        "        images.append(image)\n",
        "        boxes.append(label)\n",
        "\n",
        "    # Подготовка пакета изображений\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    # Возвращение пакета изображений и меток боксов\n",
        "    return images, boxes"
      ],
      "metadata": {
        "id": "Db83qWSTXtRO"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После предобработки в transform максимальная и минимальная ширина иммет размеры:\n",
        "\n",
        "Максимальные размеры изображения:\n",
        "Ширина: 256\n",
        "Высота: 256\n",
        "\n",
        "Минимальные размеры изображения:\n",
        "Ширина: 256\n",
        "Высота: 256\n",
        "\n",
        "Кол-во изображений с минимальными размерами: 12880\n",
        "Кол-во изображений с максимальными размерами: 12880\n"
      ],
      "metadata": {
        "id": "Udf3zsyzkNm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split, Subset\n",
        "\n",
        "# Определение размеров тренировочного, валидационного и тестового наборов данных\n",
        "train_size = int(0.6 * len(image_files))  # 60% данных для тренировки\n",
        "val_size = int(0.2 * len(image_files))  # 20% данных для валидации\n",
        "test_size = len(image_files) - train_size - val_size  # Оставшиеся 20% данных для тестирования\n",
        "\n",
        "# Разделение датасета на тренировочный, валидационный и тестовый наборы данных\n",
        "data_tr, data_val, data_test = random_split(get_data(IMAGES, LABELS), [train_size, val_size, test_size])"
      ],
      "metadata": {
        "id": "5KyAHYKp4wa-"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ppS2QA0Wd4k",
        "outputId": "36c12539-f15e-4667-afe0-08380f3f8bf2"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_tr[i][j] где i - порядковый номер картинки, j=0 - матрица картинки, j=1 - векторы координат боксов\n",
        "print(data_tr[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp4ASfcknPRL",
        "outputId": "7db12118-8ddd-47a3-a6e5-1b4509fbba66"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 0.2412109375, 0.2686781609195402, 0.1591796875, 0.16020114942528735], [0.0, 0.671875, 0.4978448275862069, 0.1298828125, 0.16163793103448276]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_size = len(data_tr)\n",
        "subset_shape = data_tr[0][0].size()\n",
        "print(f\"Размер Subset: {subset_size}\")\n",
        "print(f\"Форма первого элемента Subset: {subset_shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGGD45yEnYDl",
        "outputId": "c757d3a9-e281-4aa6-9eef-7f872ad8f2a8"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер Subset: 7728\n",
            "Форма первого элемента Subset: torch.Size([3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# кол-во изображений в тренировочном, валидационном и тестовом наборах\n",
        "len(data_tr), len(data_val), len(data_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncWpzgRU5hFq",
        "outputId": "6487e889-8453-41bf-97c9-db65a0e5591f"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7728, 2576, 2576)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kOKLO9FGuB7"
      },
      "source": [
        "### 5. Обучение моделей-детекторов (3 балла)\n",
        "\n",
        "Необходимо написать цикл обучения на PyTorch самостоятельно -- это основной критерий в этом пункте. Необходимо обучить обе выбранные модели."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# реализуем метрику IoU\n",
        "def iou_pytorch(outputs, targets, eps=1e-7):\n",
        "    # Получаем бинарные маски из выходов модели\n",
        "    outputs = (outputs > 0.5).float()\n",
        "    # Получаем площади пересечения и объединения\n",
        "    intersection = (outputs * targets).sum()\n",
        "    union = (outputs + targets).sum() - intersection\n",
        "    # Вычисляем метрику IOU\n",
        "    iou = (intersection + eps) / (union + eps)\n",
        "    return iou.item()"
      ],
      "metadata": {
        "id": "uXroPQJe6OGS"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class IoULoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred_boxes, target_boxes):\n",
        "        # Расчет координат прямоугольников\n",
        "        x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n",
        "        y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n",
        "        x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n",
        "        y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n",
        "\n",
        "        # Расчет площадей пересечения и объединения\n",
        "        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
        "        union = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1]) + \\\n",
        "                (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1]) - intersection\n",
        "\n",
        "        # Расчет индекса пересечения и объединения\n",
        "        iou = intersection / union\n",
        "\n",
        "        # Расчет функции потерь\n",
        "        loss = 1 - iou.mean()\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "eTHtl6GgGSng"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzqKTfrRkKXq",
        "outputId": "f20cb12f-477c-49fa-ee60-27ab779c12b8"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, opt, loss_fn, epochs, data_tr, data_val):\n",
        "    # Создание scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_iou = 0.0\n",
        "\n",
        "        for batch in data_tr:\n",
        "            # Распаковка пакета данных\n",
        "            batch_images, batch_boxes = batch\n",
        "\n",
        "            # Перемещение данных на устройство (если доступно)\n",
        "            if torch.cuda.is_available():\n",
        "                batch_images = batch_images.cuda()\n",
        "                batch_boxes = batch_boxes.cuda()\n",
        "\n",
        "            print(batch_images.size())\n",
        "\n",
        "\n",
        "            # Обнуление градиентов\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Прямой проход модели\n",
        "            outputs = model(batch_images)\n",
        "\n",
        "            # Расчет функции потерь\n",
        "            loss = loss_fn(outputs, batch_boxes)\n",
        "\n",
        "            # Обратный проход и оптимизация\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # Суммирование потерь\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Вычисление метрики IOU\n",
        "            iou = iou_pytorch(outputs, batch_boxes)\n",
        "            total_iou += iou\n",
        "\n",
        "        # Вычисление средней потери и метрики IOU для эпохи\n",
        "        avg_loss = total_loss / len(data_tr)\n",
        "        avg_iou = total_iou / len(data_tr)\n",
        "\n",
        "        # Применение scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Вывод результатов\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, IOU: {avg_iou:.4f}\")\n"
      ],
      "metadata": {
        "id": "4NlwqjylZRdW"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SSD(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SSD, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Feature extraction layers\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Conv1\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Conv2\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pool1\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Conv3\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),  # Conv4\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pool2\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Conv5\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Conv6\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Conv7\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pool3\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # Conv8\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Conv9\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Conv10\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pool4\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Conv11\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Conv12\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Conv13\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),  # Pool5\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),  # Conv14\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=1),  # Conv15\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Auxiliary convolution layers for detection\n",
        "        self.extra_layers = nn.ModuleList([\n",
        "            nn.Conv2d(1024, 256, kernel_size=1),  # Conv16_1\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # Conv16_2\n",
        "            nn.Conv2d(512, 128, kernel_size=1),  # Conv17_1\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # Conv17_2\n",
        "            nn.Conv2d(256, 128, kernel_size=1),  # Conv18_1\n",
        "            nn.Conv2d(128, 256, kernel_size=3),  # Conv18_2\n",
        "            nn.Conv2d(256, 128, kernel_size=1),  # Conv19_1\n",
        "            nn.Conv2d(128, 256, kernel_size=3),  # Conv19_2\n",
        "        ])\n",
        "\n",
        "        # Localization and classification layers\n",
        "        self.loc_layers = nn.ModuleList()\n",
        "        self.conf_layers = nn.ModuleList()\n",
        "\n",
        "        # The number of default boxes per feature map location\n",
        "        mbox_sizes = [4, 6, 6, 6, 4, 4]\n",
        "\n",
        "        for k, mbox_size in enumerate(mbox_sizes):\n",
        "            self.loc_layers.append(nn.Conv2d(256, mbox_size * 4, kernel_size=3, padding=1))\n",
        "            self.conf_layers.append(nn.Conv2d(256, mbox_size * num_classes, kernel_size=3, padding=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        sources = []\n",
        "        locs = []\n",
        "        confs = []\n",
        "\n",
        "        # Apply feature extraction layers\n",
        "        for i in range(23):\n",
        "            x = self.features[i](x)\n",
        "            if i in {9, 16, 23}:\n",
        "                sources.append(x)\n",
        "\n",
        "        # Apply extra layers\n",
        "        for i, layer in enumerate(self.extra_layers):\n",
        "            x = F.relu(layer(x))\n",
        "            if i % 2 == 1:\n",
        "                sources.append(x)\n",
        "\n",
        "        # Apply localization and classification layers to source layers\n",
        "        for (x, l, c) in zip(sources, self.loc_layers, self.conf_layers):\n",
        "            locs.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            confs.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "\n",
        "        # Reshape the output tensors\n",
        "        locs = torch.cat([o.view(o.size(0), -1) for o in locs], 1)\n",
        "        confs = torch.cat([o.view(o.size(0), -1) for o in confs], 1)\n",
        "\n",
        "        return locs, confs\n"
      ],
      "metadata": {
        "id": "I_kpn32e-cqb"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=SSD(num_classes=1)\n",
        "\n",
        "# Перемещение модели на устройство (если доступно)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "apUioS1sZn4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Оборачиваем данные в даталоадере\n",
        "# train_dataloader = DataLoader(data_tr, batch_size=batch_size, shuffle=True)\n",
        "train_loader = DataLoader(data_tr, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "val_dataloader = DataLoader(data_val, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "cSzT5Kz_lu-0"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRRHMKp5ENJG",
        "outputId": "55fb2add-c6a2-4e31-b653-f2ad6c4d2262"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим внутреннюю структуру пакета даталоадера\n",
        "batch = next(iter(train_loader))\n",
        "for item in batch:\n",
        "    print(item.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "euSN_8dPmimj",
        "outputId": "33628908-2c32-40a6-f5b3-f0567bcce510"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-278-8118330c5416>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "\n",
        "# Создание оптимизатора\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Создание функции потерь\n",
        "loss_fn = IoULoss()"
      ],
      "metadata": {
        "id": "YX1if1MoZZ1u"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вызов функции train\n",
        "train(model, optimizer, loss_fn, epochs, train_loader, data_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "xiNF5h_xZtUw",
        "outputId": "14aac29c-0b87-442d-f39d-8a88d11be788"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-272-d65dc32ed8df>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Вызов функции train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-266-3b779dd5c1e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, opt, loss_fn, epochs, data_tr, data_val)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Прямой проход модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Расчет функции потерь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-267-8742858a2a9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Apply extra layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 1024, 1, 1], expected input[32, 512, 32, 32] to have 1024 channels, but got 512 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XAXboBIGuB7"
      },
      "source": [
        "> Результатом выполнения пункта явлется код, запуск которого ведет к обучению модели на выбранном датасете. При обучении **обязательно выводить числовые значения лосса на трейне и валидации**, крайне желательно использовать [`TensorBoard`](https://pytorch.org/docs/stable/tensorboard.html) для визуализации. Обязательно также сохранять модель после каждой N-ой эпохи, чтобы потом ее качество можно было проверить и веса были переиспользуемыми."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyZ2CgtoGuB8"
      },
      "source": [
        "### 6. Измерение качества работы модели (метрики согласуются с руководителем и зависят от задачи) (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agZ_BOjEGuB9"
      },
      "source": [
        "Под метриками понимаются функции/формулы, по которым оценивается качество модели-детектора. Обычно для измерения качества работы детектора используют поклассовые Precision, Recall, F1-меру и mean Average Precision (mAP). Подробнее про них можно послушать в [видеолекции 2018 года](https://www.youtube.com/watch?v=ewkSI2cuyoQ&list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3&index=24).\n",
        "\n",
        "**Необходимо самостоятельно реализовать требуемые метрики!**\n",
        "\n",
        "> Результат пункта --- реализованные функции метрик для задачи детектирования, позволяющие оценить качество работы модели на выборке, а также оценка обеих обученных моделей по данным метрикам на test. Необходимо сделать вывод о том, какая модель сработала лучше и оценить полученный результат."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9IEmKseX3mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnvIv-rbGuB9"
      },
      "source": [
        "### 7. Поиск путей применения этой модели в бизнесе/реальных задачах/набросок встраивания в веб/мобильное демо (1 балл)\n",
        "\n",
        "В этом пункте нужно подумать, как эта модель может быть использована в дальнейшем. То есть, например, зачем нужно детектировать фрукты? Или одежду?\n",
        "\n",
        "> Результат пункта $-$ перечисленные кейсы использования модели (описанные **как можно подробнее**).\n",
        "\n",
        "**IMPORTANT NOTE:** Обычно этим вопросом все же задаются до начала какой-либо разработки. Но поскольку проект носит учебный/исследовательский характер, допустимо говорить об этом в конце"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4w5YtFZ0X3Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTEStsXGuBv"
      },
      "source": [
        "# Продуктовый трек"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbRJ7T6QGuBw"
      },
      "source": [
        "На этом треке вам не понадобится обучать свою модель детекции (хотя никто не запрещает вам это делать), но необходимо, во-первых, продумать **продуктовую составляющую проекта** (проблема людей, которая решается в данном проекте, целевая аудитория продукта, оптимальный способ внедрения модели), а также создать [MVP](https://ru.wikipedia.org/wiki/%D0%9C%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE_%D0%B6%D0%B8%D0%B7%D0%BD%D0%B5%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%B4%D1%83%D0%BA%D1%82) , **внедрив модель детекции в цифровой сервис**, который может быть реализован как Telegram-бот, Web-демо, Desktop-приложение.\n",
        "\n",
        "Ваша модель не обязательно должна содержать в себе лишь детекцию: например, существуют составные модели, которые осуществляют детекцию лиц на фотографии и определяют их настроение/возраст. Такие модели тоже можно и даже желательно использовать, если того требует проект. Единственное требование --- чтобы детекция присутствовала в качестве основной/вспомогательной задачи.\n",
        "\n",
        "Если у Вас есть опыт веб- или мобильной разработки, можете работать в рамках привычных Вам инструментов. Главное, чтобы в итоге они позволяил встроить в себя нейросетевой детектор, на вход которому будут поступать картинки.\n",
        "\n",
        "Изображения на вход демо могут поступать с веб-камеры, из файлов, по ссылке или с камеры мобильного телефона -- способ должен вытекать из предполагаемого сценария применения вашего продукта. Демо должно показывать, что детектор успешно отрабатывает на поданных изображениях и находит нужные объекты."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## План работы"
      ],
      "metadata": {
        "id": "vp4s0VHLZsOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Поиск проблемы и описание решения (2 балла)\n",
        "В этом пункте необходимо сформулировать проблему реального мира и продумать, как именно она будет решаться с помощью вашего продукта.\n",
        "\n",
        "#### Как должен быть устроен ваш продукт\n",
        "Здесь мы не будем подробно обсуждать, как создавать IT-продукты, которые будут пользоваться широким спросом и способны генерировать выручку. Но кратко опишем!\n",
        "\n",
        "1. **Ваш продукт должен решать существующую проблему**. Исследования показывают, что это основная причина провала стартапов --- решение не существующей проблемы. О том, как создать стартап, который решает реальную проблему пользователя, можно прочитать [здесь](https://stfalcon.com/ru/blog/post/startups-solving-user-problems). Также есть отличная книжка \"Спроси маму\", которую необходимо прочитать любому человеку, который создает свой продукт.\n",
        "\n",
        "2. **Ваш продукт должен иметь целевую аудиторию**. Этот пункт увязан с предыдущим. Если у продукта нет целевой аудитории, его никто не будет использовать.\n",
        "\n",
        "3. **Ваш продукт должен быть оформлен в сервис, подходящий для основного сценария использования продукта и целевой аудитории**. Предположим, например, что вы делаете цифровой сервис для распознавания языка жестов. Как может выглядеть такой продукт и в какой сервис он может быть внедрён? Например, если создать ТГ-бота, который будет детектировать и распознавать жест по фотографии, его довольно сложно будет использовать, потому что каждый жест в отдельности сфотографировать нельзя. Оптимальным решением в этом случае было бы мобильное приложение с потоковым детектированием жеста на видео и автоматическим добавлением субтитров. При этом именно такой продукт может быть слишком сложен в реализации. Тогда необходимо выбрать оформление сервиса, которое будет осмысленно с продуктовой точки зрения и которое вы при этом сможете реализовать.\n",
        "\n",
        "#### Как искать проблему\n",
        "Есть много способов найти важную и актуальную проблему. Некоторые советы перечислены в книге \"Спроси маму\". Несколько коротких советов можно найти [здесь](https://vc.ru/life/1735-startup-ideas).\n",
        "* Можно подумать о темах, которые близки лично вам/вашим знакомым. Если проект решает проблему даже узкой целевой аудитории, это не страшно.\n",
        "* Можно найти уже существующий проект и улучшить его, обозначив, в чем преимущество вашего решения перед конкурентами.\n",
        "* У человечества вообще много глобальных [проблем](https://www.un.org/sustainabledevelopment/ru/sustainable-development-goals/), над которыми борются различные мировые организации. Если ваш проект способен хоть в каком-то частном случае продвинуться к решению этих проблем, это уже будет отлично.\n",
        "* Для поиска идей можно использовать датасеты с kaggle.\n",
        "\n",
        "> Результат пункта -- подробное описание проблемы, которую вы решаете, целевая аудитория использования продукта, а также **подробное** описание сервиса, который предлагается создать. Допускается описать \"идеальный продукт\", а затем создать MVP, имеющий отклонения от оптимального варианта, сославшись на ограниченное время/ресурсы для выполнения проекта. Но тогда это необходимо отдельно упомянуть в этом пункте."
      ],
      "metadata": {
        "id": "fTi_YuevZxqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hnfqSran-TZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Поиск обученной модели и датасета (1 балл)\n",
        "\n",
        "В этом пункте вам необходимо выбрать модель, которую вы встроите в ваш продукт, и датасет, на котором вы эту модель будете тестировать.\n",
        "\n",
        "* Если вы найдете готовую модель, которую можно применить для вашей задачи, можно просто взять её. В этом случае с датасетом можно особо не заморачиваться. Достаточно в этом пункте запустить ваш детектор на нескольких (5-7) изображениях, на которых модель будет в итоге применяться, и проверить, что модель на них хорошо работает.\n"
      ],
      "metadata": {
        "id": "nFRfmcVF4YrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mx3Gncop-VLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Бонус. 2.5. Обучение модели для вашей задачи (5 баллов)\n",
        "**За этот пункт ставятся бонусные баллы. Он не является обязательным.**\n",
        "\n",
        "Если готовой обученной модели вы не смогли найти, тогда необходимо будет обучить модель самостоятельно. В таком случае перед выполнением пункта 2 вам необходимо будет найти подходящий датасет и обучить любую модель детекции с помощью встроенных методов из фреймворков, описанных в первом сценарии:\n",
        "- `torchvision.models.detection` и `torchhub`: \"нативные\" модели для детектирования прямо из PyTorch. Примеры использования есть прямо на занятиях DLSchool по практике CV [2019 года](https://www.youtube.com/watch?v=XSPYe4-y4HE) и [2020 года](https://stepik.org/lesson/458313/step/1?unit=616131);\n",
        "- `mmdetection`: как с ним работать, рассказывается в [практическом занятии](https://stepik.org/lesson/458313/step/2?unit=616131).\n",
        "- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (\"Quick Start\");\n",
        "- `TensorFlow Object Detection API`: как с ним работать рассказывается в [занятии 2018 года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Работать предстоит с [официальным репозиторием](https://github.com/tensorflow/models/tree/master/research/object_detection).\n",
        "\n",
        "После обучения модель нужно будет протестировать на real-world изображениях, на которых планируется использовать продукт."
      ],
      "metadata": {
        "id": "lGG31_2V7zo7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgTjikcG-Vx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lczuLKgpGuBx"
      },
      "source": [
        "### 3. Выбор фреймворка/библиотеки для разработки веб/мобильного демо (0 баллов)\n",
        "\n",
        "Основным инструментом для разработки веб-демо будет микрофреймворк **Flask**: [серия туториалов на русском](https://habr.com/ru/post/346306/).   \n",
        "Полезные ресуры:\n",
        "- [курс по веб-разработке](https://www.youtube.com/playlist?list=PLzQrZe3EemP5KsgWGnmC0QrOzQqjg3Kd5), нас интересуют первые 7 видео в плейлисте. В частности, нужны видео по Flask, там очень хорошие обучалки параллельно с лектором;\n",
        "- [исчерпывающий справочник по Flask (англ)](https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python3_Flask.html);\n",
        "- можно посмотреть мой [репозиторий с реализацией веб-демо](https://github.com/izaharkin/Respalyzer) для ML-задачи оценки отзывов.\n",
        "\n",
        "Для разработки мобильного демо стоит выбрать инстурмент на свое усмотрение:\n",
        "- под Android: [пример на Pytorch Mobile](https://towardsdatascience.com/object-detector-android-app-using-pytorch-mobile-neural-network-407c419b56cd), [пример на TensorFlow Lite](https://www.tensorflow.org/lite/models/object_detection/overview). **Примечание** от Дмитрия Шумилина: на Android с TF Lite на момент января 2021 есть [ошибка](https://github.com/tensorflow/models/issues/9341) с новым форматом хранения модели. Можно попробовать возможное [решение](https://www.youtube.com/watch?v=syTKGY-H44E&ab_channel=DoomsdayRobotics) или писать на чистом Java. Также можно попробовать использовать более старые версии TensorFlow, в которых проблем совместимости еще не было, например, [v2.1.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0).\n",
        "- под iOS: [пример на TensorFlow Lite Swift API](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift), [пример с Vision Framework](https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture) на \"чистом\" Swift'е.\n",
        "\n",
        "Разумеется, лучше **самостоятельно поискать видео/статьи** на тему использования моделей на мобильных устройствах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpGAvez5GuBx"
      },
      "source": [
        "> Результатом пункта является зафисированный для вас инструмент для разработки демо."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oP_hQjnF-Zrx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggyC5X6nGuBy"
      },
      "source": [
        "### 4. Разработка демо (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqsnd7mkGuBy"
      },
      "source": [
        "Этот пункт про сам процесс написания кода для демо.\n",
        "\n",
        "> Результатом пункта является код, который можно запустить. Не хватать будет только логики детектора, сам интерфейс должен быть уже рабочим."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YvdSnglS-ah1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HbHX0oAGuBz"
      },
      "source": [
        "### 5. Встраивание модели-детектора в демо (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUgXk1UBGuB0"
      },
      "source": [
        "Этот пункт про процесс дописывания кода, который будет обеспечивать \"логику\" демо $-$ само детектирование.\n",
        "\n",
        "> Результатом пункта является код, который можно запустить и продемонстрировать работающую систему детектирования объектов."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dtuhzSzc-bav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC57dH9sGuB0"
      },
      "source": [
        "### 6. Тестирование демо (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbuoADSeGuB1"
      },
      "source": [
        "Здесь нужно запустить ваше демо на как можно большем количестве примеров, чтобы понять, в чем его сильные и слабые стороны. То есть какие объекты/сцены детектор обрабатывает легко, а с какими ему справится сложно. Нужно предложить также пути для улучшения модели на основе увиденных ошибок."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-_tW90YGuB1"
      },
      "source": [
        "> Результатом пункта является набор изображений, на которых демо отработало. Для каждого изображения нужно добавить комментарии, почему модель справилась хорошо/плохо, предложить пути ее улучшения."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zz7kfX0J-cJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_AQ1MMGuB2"
      },
      "source": [
        "### 7. Улучшение дизайна / Развертывание демо на сервере (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTEuUxKGuB3"
      },
      "source": [
        "В этом пункте можно пойти двумя путями:\n",
        "1. Проделать работу по улучшению визуальной составляющей демо (интерфейс)\n",
        "2. Загрузить модель на какой-нибудь сервер/хост/test-flight (в случае мобильного iOS-демо), чтобы к демо можно было обратиться прямо в адресной строке браузера / найти в Telegram\n",
        "\n",
        "\\> По *первому пункту* могу посоветовать использовать библиотеку [Bootstrap](https://habr.com/ru/post/349060/), для мобильного демо элементы UI/UX являются частью основной разработки (поэтому стоит просто погуглить/почитать документацию).\n",
        "\n",
        "\\> *Второй пункт - в случае веб-демо*:\n",
        "\n",
        "Способ 1: Google Cloud Engine.\n",
        "\n",
        "Если ваше приложение требует установки системных пакетов, например, через `apt-get install`, то вам придется работать на выделенном сервере VPN или на виртуальной машине. К счастью тот же [Google Cloud](https://cloud.google.com/compute) предоставляет бесплатные 300$ на 90 дней использования Виртуальной машиной, чего хватит в большинстве случаев. Эти ссылки помогут вам понять, как в таком случае создать виртуальную машину, установить и настроить виртуальное окружение и вебсервер, а также задеплоить проект:\n",
        "\n",
        "- [Deploying a Flask app to a Virtual Machine](https://www.youtube.com/watch?v=a2g9pDleGQk&ab_channel=JulianNash)\n",
        "- [Set up Gunicorn and Nginx](https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-20-04-ru)\n",
        "\n",
        "Способ 2: Heroku.\n",
        "\n",
        "Если с GCE проблемы/не хочется привязывать карту и т.д., могут помочь эти ресурсы и сервис [Heroku](https://www.heroku.com/):\n",
        "- [Flask deployment](http://www.tutorialspoint.com/flask/flask_deployment.htm)\n",
        "- [Deploy Flask app to Heroku (youtube)](https://www.youtube.com/watch?v=pmRT8QQLIqk)\n",
        "- [Deploy Flask app to Heroku (medium)](https://medium.com/the-andela-way/deploying-your-flask-application-to-heroku-c99050bce8f9)\n",
        "- [Set your own domain name on Heroku](https://devcenter.heroku.com/articles/custom-domains)\n",
        "\n",
        "\\> *Второй пункт - в случае мобильного демо*:\n",
        "\n",
        "Здесь с как таковым деплоем сложнее, обычно мобильные приложения публикуются или в Google Play (Android), или в AppStore (iOS). Однако можно снять **видеопоказ экрана (скринкаст)** с использованием написанного приложения -- вполне подойдет для публичной демонстрации.\n",
        "\n",
        "> Результат пункта --- видео с описанием продукта и демонстрацией работы сервиса, который развернут в интернете и доступен для использования. Также необходимо кинуть ссылку на сам сервис, если удалось его развернуть."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggcKq6uf-eca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G50tgxC9GuB-"
      },
      "source": [
        "<h2 style=\"text-align: center;\"><b>Критерии оценивания</b></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt7FI0JdGuB-"
      },
      "source": [
        "* 1 пункт $-$ 1 балл\n",
        "* 2 пункт $-$ 1 балл\n",
        "* 3 пункт $-$ 0 баллов (промежуточный пункт)  \n",
        "* 4 пункт $-$ 3 балла   \n",
        "* 5 пункт $-$ 3 балла   \n",
        "* 6 пункт $-$ 1 балл\n",
        "* 7 пункт $-$ 1 балл\n",
        "* Максимум баллов по проекту $-$ 10  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZxROjYgGuB_"
      },
      "source": [
        "**Успехов в выполнении проекта!**\n",
        "\n",
        "Желаю всем проделать полезную, интересную и качественную работу, которую потом нестыдно и в резюме указать, и друзьям показать ;)"
      ]
    }
  ]
}